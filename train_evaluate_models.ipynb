{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-06 17:26:08.604130: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/usr/lib/python3.10/site-packages/requests/__init__.py:109: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (None)/charset_normalizer (3.0.1) doesn't match a supported version!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-06 17:26:09.684745: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-06 17:26:09.697196: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-06 17:26:09.697322: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-06 17:26:09.697794: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-06 17:26:09.699330: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-06 17:26:09.699429: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-06 17:26:09.699502: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-06 17:26:09.947857: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-06 17:26:09.948000: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-06 17:26:09.948084: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-06 17:26:09.948153: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6997 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2023-01-06 17:26:09.948330: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import seaborn as sns\n",
    "\n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = \"{:.1f}\".format\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "tf.config.run_functions_eagerly(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_BT = True\n",
    "\n",
    "\n",
    "DATASET_ROOT=\"data/\"\n",
    "TRAIN_LOCATIONS_FILE=\"SignatureLocs_altered.csv\"\n",
    "GENERATED_TRAIN_LOCATIONS_FILE=\"SignatureLocs_altered_generated.csv\"\n",
    "TEST_LOCATIONS_FILE=\"TestLocs_altered.csv\"\n",
    "TRAIN_STRENGTHS_FILE=\"P_Signatures.csv\" if USE_BT else \"P_SA_Signatures.csv\"\n",
    "GENERATED_TRAIN_STRENGTHS_FILE=\"P_Signatures_generated.csv\" if USE_BT else \"P_SA_Signatures_generated.csv\"\n",
    "TEST_STRENGTHS_FILE=\"P_Tests.csv\" if USE_BT else \"P_SA_Tests.csv\"\n",
    "NUMBER_OF_BEACONS=57 if USE_BT else 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_strengths = pd.read_csv(DATASET_ROOT+TRAIN_STRENGTHS_FILE, sep=';', names=[x for x in range(NUMBER_OF_BEACONS)])\n",
    "df_train_locs = pd.read_csv(DATASET_ROOT+TRAIN_LOCATIONS_FILE, sep=';', names=['x','y'], dtype=float)\n",
    "df_generated_train_strengths = pd.read_csv(DATASET_ROOT+TRAIN_STRENGTHS_FILE, sep=';', names=[x for x in range(NUMBER_OF_BEACONS)])\n",
    "df_generated_train_locs = pd.read_csv(DATASET_ROOT+TRAIN_LOCATIONS_FILE, sep=';', names=['x','y'], dtype=float)\n",
    "df_test_strengths = pd.read_csv(DATASET_ROOT+TEST_STRENGTHS_FILE, sep=';', names=[x for x in range(NUMBER_OF_BEACONS)])\n",
    "df_test_locs = pd.read_csv(DATASET_ROOT+TEST_LOCATIONS_FILE, sep=';', names=['x','y'], dtype=float)\n",
    "\n",
    "train_features = df_train_strengths\n",
    "train_target = df_train_locs\n",
    "generated_train_features = df_train_strengths\n",
    "generated_train_target = df_train_locs\n",
    "test_features = df_test_strengths\n",
    "test_target = df_test_locs\n",
    "\n",
    "normalization_values = np.array(train_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define all models\n",
    "Currently:\n",
    "- 3 hidden layer mixed with dropout layers (basic_model) (deep-learn-paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_basic_model():\n",
    "    normalizer = layers.Normalization()\n",
    "    normalizer.adapt(normalization_values)\n",
    "\n",
    "    model = tf.keras.models.Sequential([\n",
    "      normalizer,\n",
    "      layers.Dense(500, activation='relu'),\n",
    "      layers.Dropout(0.5),\n",
    "      layers.Dense(500, activation='relu'),\n",
    "      layers.Dropout(0.5),\n",
    "      layers.Dense(2, activation='relu'),\n",
    "      layers.Dropout(0.5)\n",
    "    ])\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    loss_function = tf.keras.losses.MeanAbsoluteError()\n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                    loss=loss_function)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define more models\n",
    "def gen_simple_model():\n",
    "    normalizer = layers.Normalization()\n",
    "    normalizer.adapt(normalization_values)\n",
    "\n",
    "    model = tf.keras.models.Sequential([\n",
    "      normalizer,\n",
    "      layers.Dense(math.ceil((2//3) * NUMBER_OF_BEACONS), activation='relu'),\n",
    "      layers.Dropout(0.5),\n",
    "      layers.Dense(math.ceil((2//3) * NUMBER_OF_BEACONS), activation='relu'),\n",
    "      layers.Dropout(0.5),\n",
    "      layers.Dense(2, activation='relu'),\n",
    "      layers.Dropout(0.5)\n",
    "    ])\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    loss_function = tf.keras.losses.MeanAbsoluteError()\n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                    loss=loss_function)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paper: Improved Indoor Geomagnetic Field Fingerprinting for Smartwatch Localization Using Deep Learning\n",
    "def homayani_conv_model():\n",
    "    normalizer = layers.Normalization()\n",
    "    normalizer.adapt(normalization_values)\n",
    "\n",
    "    model = tf.keras.models.Sequential([\n",
    "      normalizer,\n",
    "      layers.Reshape((NUMBER_OF_BEACONS, 1)),\n",
    "      layers.Conv1D(16, 3, activation='relu'),\n",
    "      layers.Conv1D(32, 3, activation='relu'),\n",
    "      layers.Dense(603, activation='relu'), # Equal to measured locations\n",
    "      layers.Flatten(),\n",
    "      layers.Dense(2, activation='relu')\n",
    "    ])\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    loss_function = tf.keras.losses.MeanAbsoluteError()\n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                    loss=loss_function)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paper: A Comparison Analysis of BLE-Based Algorithms forLocalization in Industrial Environments\n",
    "def cannizzaro_mlp():\n",
    "    normalizer = layers.Normalization()\n",
    "    normalizer.adapt(normalization_values)\n",
    "\n",
    "    model = tf.keras.models.Sequential([\n",
    "      normalizer,\n",
    "      layers.Dense(8), # Input after normalization\n",
    "      layers.Dense(8, activation='relu'),\n",
    "      layers.Dense(6, activation='relu'),\n",
    "      layers.Dense(2, activation='relu'),\n",
    "    ])\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    loss_function = tf.keras.losses.MeanAbsoluteError()\n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                    loss=loss_function)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paper: CNNLoc: Deep-Learning Based Indoor Localization with WiFi Fingerprinting\n",
    "def cnn_loc():\n",
    "    normalizer = layers.Normalization()\n",
    "    normalizer.adapt(normalization_values)\n",
    "\n",
    "    model = tf.keras.models.Sequential([\n",
    "      normalizer,\n",
    "      layers.Dense(math.ceil(NUMBER_OF_BEACONS * 2.5), activation='elu'),\n",
    "      layers.Dense(math.ceil(NUMBER_OF_BEACONS * 1.25), activation='elu'),\n",
    "      layers.Reshape((math.ceil(NUMBER_OF_BEACONS * 1.25), 1)),\n",
    "      layers.Dropout(0.7),\n",
    "      layers.Conv1D(math.ceil(NUMBER_OF_BEACONS * 0.5), math.ceil(NUMBER_OF_BEACONS * 0.12)),\n",
    "      layers.Conv1D(math.ceil(NUMBER_OF_BEACONS * 0.4), math.ceil(NUMBER_OF_BEACONS * 0.12)),\n",
    "      layers.Conv1D(math.ceil(NUMBER_OF_BEACONS * 0.3), math.ceil(NUMBER_OF_BEACONS * 0.12)),\n",
    "      layers.Flatten(),\n",
    "      layers.Dense(2, activation='elu')\n",
    "    ])\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    loss_function = tf.keras.losses.MeanAbsoluteError()\n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                    loss=loss_function)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate all models in the same way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_generators = [\n",
    "    (gen_basic_model, 'Basic model, 3 hidden layers (500) with dropout (0.5)'),\n",
    "    (gen_simple_model, 'Simple model, 3 hidden layers 2/3 * #_{APs} with dropout (0.5)'),\n",
    "    (homayani_conv_model, 'Basic 1D convolutional network, 2 conv layers and 1 dense'),\n",
    "    (cannizzaro_mlp, 'Simple MLP, 3 hidden layers (8, 8, 6)'),\n",
    "    (cnn_loc, 'CNNLoc, SAE + Conv layers'),\n",
    "]\n",
    "EPOCHS = 30000\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "\n",
    "models_default = [m[0]() for m in model_generators]\n",
    "models_generated = [m[0]() for m in model_generators]\n",
    "\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=1000)\n",
    "for i, model in enumerate(models_default):\n",
    "    print(f\"Training model '{model_generators[i][1]}' on original data\")\n",
    "    model.fit(train_features, train_target, epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=[es])\n",
    "\n",
    "for i, model in enumerate(models_generated):\n",
    "    print(f\"Training model '{model_generators[i][1]}' on generated data\")\n",
    "    model.fit(train_features, train_target, epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating original data model 'Basic model, 3 hidden layers (500) with dropout (0.5)'\n",
      "15/15 [==============================] - 0s 831us/step - loss: 218.6404\n",
      "Evaluating generated data model 'Basic model, 3 hidden layers (500) with dropout (0.5)'\n",
      "15/15 [==============================] - 0s 907us/step - loss: 210.2543\n"
     ]
    }
   ],
   "source": [
    "for i, model in enumerate(models_default):\n",
    "    print(f\"Evaluating original data model '{model_generators[i][1]}'\")\n",
    "    model.evaluate(test_features, test_target)\n",
    "\n",
    "for i, model in enumerate(models_generated):\n",
    "    print(f\"Evaluating generated data model '{model_generators[i][1]}'\")\n",
    "    model.evaluate(test_features, test_target)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
